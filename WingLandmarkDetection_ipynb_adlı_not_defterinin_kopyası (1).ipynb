{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8CFvj9dTYU",
        "outputId": "c8213dfa-846c-433d-d58d-05bc7b60e647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciMEI6iMdVxT"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/gdrive/MyDrive/Bee/ASAS_ACORES_2017.zip #old\n",
        "!unzip /content/gdrive/MyDrive/Bee/images_manuel.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHly_r1meej1"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/gdrive/MyDrive/Bee/masks.zip #old\n",
        "!unzip /content/gdrive/MyDrive/Bee/masks_manuel.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm7vUNNkb4fs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGhSKLKEcSh-"
      },
      "source": [
        "# Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWT_fMQNcEwv"
      },
      "outputs": [],
      "source": [
        "class WingDataset(Dataset):\n",
        "    def __init__(self, X_train, Y_train, transforms=None, target_transform=None):\n",
        "        self.targets = Y_train #vector of (batch_size, 3, h, w)\n",
        "        self.input = X_train # image with size (batch_size, 3, h, w)\n",
        "        self.transform = transforms\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.input[idx]\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        data = torch.tensor(data)\n",
        "        target = torch.tensor(target)\n",
        "\n",
        "        return data, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFLOJJFYceMg"
      },
      "source": [
        "# Preprocess the data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape(img):\n",
        "\n",
        "\tdesired_size = 400 #novo tamanho de imagem\n",
        "\t\n",
        "\timg = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
        "\t#img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\t\n",
        "\tprint(img.shape)\n",
        "\trows, cols = img.shape\n",
        "\n",
        "\n",
        "\told_size = img.shape[:2] # old_size is in (height, width) format\n",
        "\n",
        "\tratio = float(desired_size)/max(old_size)\n",
        "\tnew_size = tuple([int(x*ratio) for x in old_size])\n",
        "\n",
        "\t# new_size should be in (width, height) format\n",
        "\timg = cv2.resize(img, (new_size[1], new_size[0]))\n",
        "\n",
        "\tdelta_w = desired_size - new_size[1]\n",
        "\tdelta_h = desired_size - new_size[0]\n",
        "\ttop, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "\tleft, right = delta_w//2, delta_w-(delta_w//2)\n",
        "\n",
        "\tcolor = [0, 0, 0]\n",
        "\tnew_im = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "\t\n",
        "\treturn new_im"
      ],
      "metadata": {
        "id": "RSTlwBfVFfda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsyF9YNlcgPR"
      },
      "outputs": [],
      "source": [
        "def preprocess(data_path='/content/images', label_path=\"/content/masks\"):\n",
        "    current_path = os.getcwd()  \n",
        "    print(\"Preprocessing...\")\n",
        "    dataset = []\n",
        "    targets = []\n",
        "\n",
        "    cannotFind = 0\n",
        "    for i in range(len(os.listdir(os.path.join(current_path,data_path)))):\n",
        "        file_name = os.listdir(data_path)[i]\n",
        "        # print(file_name)\n",
        "        image_path = os.path.join(data_path, file_name)\n",
        "        target_path = os.path.join(label_path, file_name)\n",
        "        # try:\n",
        "            # img = Image.open(image_path)\n",
        "            # target = Image.open(target_path)\n",
        "        if not os.path.isfile(target_path):\n",
        "            print(\"Could not find the image {}, current missing image count is {}.\".format(target_path, cannotFind+1))\n",
        "            cannotFind += 1\n",
        "            continue\n",
        "        img = cv2.imread(image_path)\n",
        "        img = reshape(img)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        target = cv2.imread(target_path, cv2.IMREAD_GRAYSCALE)\n",
        "        dataset.append(img)\n",
        "        targets.append(target)\n",
        "        # except:\n",
        "        #     print(\"Could not find the image {}, current missing image count is {}.\".format(target_path, cannotFind+1))\n",
        "        #     cannotFind += 1\n",
        "\n",
        "        \n",
        "    # dataset = np.array(dataset)\n",
        "    # print(dataset.shape)\n",
        "\n",
        "\n",
        "    return dataset,targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMU70q7nepKR"
      },
      "source": [
        "# Create Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhKWkZMnezZz"
      },
      "outputs": [],
      "source": [
        "images, targets = preprocess()\n",
        "\n",
        "# print(coordinates)\n",
        "# create own Dataset\n",
        "dataset = WingDataset(images,\n",
        "                    targets,\n",
        "                    transforms = torchvision.transforms.Compose([\n",
        "                                torchvision.transforms.ToPILImage(),\n",
        "                                torchvision.transforms.Resize(400),\n",
        "                                torchvision.transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "                                torchvision.transforms.RandomApply(\n",
        "                                    transforms = [torchvision.transforms.GaussianBlur(kernel_size=(1, 3), sigma=(0.1, 2))],\n",
        "                                    p = 0.5\n",
        "                                ),\n",
        "                                torchvision.transforms.RandomEqualize(p=0.15),\n",
        "                                torchvision.transforms.RandomAutocontrast(),\n",
        "                                torchvision.transforms.ToTensor()]),\n",
        "\n",
        "                    target_transform = torchvision.transforms.Compose([\n",
        "                                torchvision.transforms.ToPILImage(),\n",
        "                                torchvision.transforms.Resize(400),\n",
        "                                torchvision.transforms.ToTensor()])\n",
        "                )\n",
        "\n",
        "\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "# Batch size\n",
        "train_batch_size = 1\n",
        "\n",
        "train_length=int(0.80* len(dataset))\n",
        "test_length=len(dataset)-train_length\n",
        "\n",
        "train_dataset,test_dataset=torch.utils.data.random_split(dataset,(train_length,test_length))\n",
        "\n",
        "\n",
        "dataloader_train=torch.utils.data.DataLoader(train_dataset,\n",
        "                                        batch_size=train_batch_size,\n",
        "                                        shuffle=True,\n",
        "                                        collate_fn=collate_fn)\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(test_dataset,\n",
        "                                            batch_size=train_batch_size,\n",
        "                                            shuffle=True,\n",
        "                                            collate_fn = collate_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "680ctb1me5gu"
      },
      "source": [
        "# Download the UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhs8RJ76e9Jr",
        "outputId": "a5f28253-3d3d-4d21-a236-966c8fffa734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda\n",
            "UNet(\n",
            "  (encoder1): Sequential(\n",
            "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc1relu1): ReLU(inplace=True)\n",
            "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc1relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder2): Sequential(\n",
            "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc2relu1): ReLU(inplace=True)\n",
            "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc2relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder3): Sequential(\n",
            "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc3relu1): ReLU(inplace=True)\n",
            "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc3relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (encoder4): Sequential(\n",
            "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc4relu1): ReLU(inplace=True)\n",
            "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (enc4relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bottleneck): Sequential(\n",
            "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bottleneckrelu1): ReLU(inplace=True)\n",
            "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bottleneckrelu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder4): Sequential(\n",
            "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec4relu1): ReLU(inplace=True)\n",
            "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec4relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder3): Sequential(\n",
            "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec3relu1): ReLU(inplace=True)\n",
            "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec3relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder2): Sequential(\n",
            "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec2relu1): ReLU(inplace=True)\n",
            "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec2relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "  (decoder1): Sequential(\n",
            "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec1relu1): ReLU(inplace=True)\n",
            "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (dec1relu2): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(\"device is {}\".format(device))\n",
        "\n",
        "\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=False)\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     print(param.requires_grad)\n",
        "\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUzvjPHBkezI"
      },
      "source": [
        "# Accuracy Calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieLtIwHjkhRy"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "def accuracy_check(mask, prediction, print_every, verbose=False):\n",
        "    # ims = [mask, prediction]\n",
        "    np_ims = []\n",
        "    # probs = nn.functional.sigmoid(prediction)\n",
        "    probs = prediction>0.90\n",
        "    mask = mask>0.9\n",
        "    ims = [mask,probs]\n",
        "    # print(\"ALOO\", (mask))\n",
        "    # print(\"ALOO2\", probs)\n",
        "    for item in ims:\n",
        "        if 'str' in str(type(item)):\n",
        "            item = np.array(Image.open(item))\n",
        "        elif 'PIL' in str(type(item)):\n",
        "            item = np.array(item)\n",
        "        elif 'torch' in str(type(item)):\n",
        "            item = item.detach().cpu().numpy()\n",
        "        np_ims.append(item)\n",
        "\n",
        "    if verbose:\n",
        "        fig = plt.figure()\n",
        "        fig.add_subplot(1, 2, 1)\n",
        "        plt.imshow(np_ims[0][0], interpolation='nearest')\n",
        "        plt.title(\"correct landmarks\")\n",
        "        fig.add_subplot(1, 2, 2)\n",
        "        plt.imshow(np_ims[1][0], interpolation='nearest')\n",
        "        plt.title(\"prediction\")\n",
        "        plt.show()\n",
        "\n",
        "    white_points = np.sum(np_ims[0])\n",
        "    \n",
        "    w_b = np.sum(np.greater(np_ims[0], np_ims[1]))\n",
        "    b_w = np.sum(np.less(np_ims[0], np_ims[1]))\n",
        "    eq = np.sum(np.equal(np_ims[0], np_ims[1]))\n",
        "   \n",
        "    if print_every:\n",
        "        print(\"White point count: \", white_points, \" White - Black: \", w_b,\n",
        "             \" Black - White: \", b_w , \" Equal pixels : \", eq)\n",
        "    \n",
        "\n",
        "    # accuracy = np.sum(np_ims[1])\n",
        "\n",
        "    white_accuracy_point = ((white_points-w_b)/white_points)*100\n",
        "    if print_every:\n",
        "        print(\"gained accuracy for correct white predictions: \",white_accuracy_point)\n",
        "    wrong_deduction = (b_w**2)*0.00002 # point deduction from wrong predictions\n",
        "    if print_every:\n",
        "        print(\"deduction acc from wrong black predictions: \", wrong_deduction)\n",
        "    return white_accuracy_point-wrong_deduction\n",
        "\n",
        "\n",
        "def accuracy_check_for_batch(masks, predictions, batch_size, print_every, verbose=False):\n",
        "    total_acc = 0\n",
        "    for index in range(batch_size):\n",
        "        # print(masks.shape)\n",
        "        # print(predictions.shape)\n",
        "        total_acc += accuracy_check(masks[index], predictions[index], print_every, verbose)\n",
        "    return total_acc/batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs7QrcAkelD7"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NvCLnn9xz9Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all batches, or for a single mask\n",
        "    assert input.size() == target.size()\n",
        "    if input.dim() == 2 and reduce_batch_first:\n",
        "        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')\n",
        "\n",
        "    if input.dim() == 2 or reduce_batch_first:\n",
        "        inter = torch.dot(input.reshape(-1), target.reshape(-1))\n",
        "        sets_sum = torch.sum(input) + torch.sum(target)\n",
        "        if sets_sum.item() == 0:\n",
        "            sets_sum = 2 * inter\n",
        "\n",
        "        return (2 * inter + epsilon) / (sets_sum + epsilon)\n",
        "    else:\n",
        "        # compute and average metric for each batch element\n",
        "        dice = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            dice += dice_coeff(input[i, ...], target[i, ...])\n",
        "        return dice / input.shape[0]\n",
        "\n",
        "\n",
        "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):\n",
        "    # Average of Dice coefficient for all classes\n",
        "    assert input.size() == target.size()\n",
        "    dice = 0\n",
        "    for channel in range(input.shape[1]):\n",
        "        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)\n",
        "\n",
        "    return dice / input.shape[1]\n",
        "\n",
        "\n",
        "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
        "    # Dice loss (objective to minimize) between 0 and 1\n",
        "    assert input.size() == target.size()\n",
        "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
        "    return 1 - fn(input, target, reduce_batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKKsQUqhenJl"
      },
      "outputs": [],
      "source": [
        "import time \n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, epochs=20):\n",
        "    pos_weight = torch.ones(400).cuda()\n",
        "    pos_weight = pos_weight*50\n",
        "    loss_func = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training Starts...\")\n",
        "    #####################################################################3\n",
        "    #Training\n",
        "\n",
        "    best_acc = -100.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        temp_accuracy = []\n",
        "        plot_acc = []\n",
        "        plot_starting_batch=0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(dataloader_train):\n",
        "            # print(inputs.shape,labels.shape)\n",
        "            inputs = torch.stack(inputs)\n",
        "            plt.imshow(inputs[0].permute(1,2,0))\n",
        "            plt.plot()\n",
        "            labels = torch.stack(labels)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)      \n",
        "               \n",
        "            # labels = torch.argmax(labels, dim=1)\n",
        "            # print(torch.unique(labels))   \n",
        "\n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # temp_inp = reshape(inputs.cpu())\n",
        "            # plt.plot(inputs.cpu())\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "  \n",
        "            # Compute loss\n",
        "            loss = loss_func(outputs.float(), labels.float())  \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Compute the accuracy\n",
        "            if(i%100 == 0):\n",
        "                print_every = True\n",
        "            else:\n",
        "                print_every = False\n",
        "\n",
        "            acc = accuracy_check_for_batch(labels,outputs,train_batch_size,print_every,verbose=True)\n",
        "            temp_accuracy.append(acc)\n",
        "            \n",
        "            if(i == 500):\n",
        "                plt.scatter(list(range(plot_starting_batch,i)),plot_acc)\n",
        "                plt.title('Training Accuracy over Batch')\n",
        "                plt.xlabel('Batch')\n",
        "                plt.ylabel('Accuracy')\n",
        "                plt.show()\n",
        "                plot_acc = []\n",
        "                plot_starting_batch=500\n",
        "            elif i == 1200:\n",
        "                print(len(plot_acc))\n",
        "                plt.scatter(list(range(plot_starting_batch,i)),plot_acc)\n",
        "                plt.title('Training Accuracy over Batch')\n",
        "                plt.xlabel('Batch')\n",
        "                plt.ylabel('Accuracy')\n",
        "                plt.show()\n",
        "                plot_acc = []\n",
        "                plot_starting_batch=1200\n",
        "            plot_acc.append(acc)\n",
        "\n",
        "            if print_every:\n",
        "                print(\"Epoch : {}, Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(epoch,i, loss.item(), acc))\n",
        "\n",
        "        accuracy = np.array(temp_accuracy).mean()\n",
        "        plt.scatter(list(range(plot_starting_batch,i+1)),plot_acc)\n",
        "        plt.title('Training Accuracy over Batch')\n",
        "        plt.xlabel('Batch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.show()\n",
        "        print(\"End of epoch: {}, Accuracy is {}.\".format(epoch,accuracy))\n",
        "        train_acc = accuracy\n",
        "\n",
        "        temp_accuracy = []\n",
        "        #Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            #Validation loop\n",
        "            for j, (inputs, labels) in enumerate(dataloader_test):\n",
        "                \n",
        "                inputs = torch.stack(inputs)\n",
        "                labels = torch.stack(labels)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)  \n",
        "                \n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "                # outputs = torch.argmax(outputs, dim=1)\n",
        "                # preds = outputs > 0.0\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_func(outputs, labels) \n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                if(i%100 == 0):\n",
        "                    print_every = True\n",
        "                else:\n",
        "                    print_every = False\n",
        "\n",
        "                acc = accuracy_check_for_batch(labels,outputs,train_batch_size, print_every, verbose=True)\n",
        "                temp_accuracy.append(acc)\n",
        "            \n",
        "                if print_every:\n",
        "                    print(\"Epoch: {}, Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(epoch,j, loss.item(), acc))\n",
        "            accuracy = np.array(temp_accuracy).mean()\n",
        "            plt.scatter(list(range(0,j+1)),temp_accuracy)\n",
        "            plt.title('Validation Accuracy over Batch, Accuracy: {:.4f}, Epoch: {}'.format(accuracy,epoch))\n",
        "            plt.xlabel('Batch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.show()\n",
        "            \n",
        "            print(\"End of epoch: {}, Test Accuracy is {}.\".format(epoch,accuracy))\n",
        "            valid_acc = accuracy\n",
        "            if accuracy > best_acc:\n",
        "                print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, accuracy))\n",
        "                best_acc = accuracy\n",
        "                torch.save(model.state_dict(), \"/content/model-acc{:.4f}-e{}.pt\".format(accuracy, epoch))\n",
        "       # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_length \n",
        "        avg_train_acc = train_acc/train_length\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_valid_loss = valid_loss \n",
        "        avg_valid_acc = valid_acc\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))        \n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgIApkjDlBVC"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ036LxplA2f"
      },
      "outputs": [],
      "source": [
        "UNet = train(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnAMr0Kc2pzb"
      },
      "outputs": [],
      "source": [
        "1model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=False)\n",
        "model.cuda()\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Bee/model-acc97.5291-e14.pt\"))\n",
        "\n",
        "for j, (inputs, labels) in enumerate(dataloader_test):\n",
        "    \n",
        "    inputs = torch.stack(inputs)\n",
        "    labels = torch.stack(labels)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)  \n",
        "    \n",
        "\n",
        "    # Forward pass - compute outputs on input data using the model\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    print_every = True\n",
        "    acc = accuracy_check_for_batch(labels,outputs,train_batch_size, print_every, verbose=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QN1eQqM2wvIT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}